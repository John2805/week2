{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ed8db3-c2ea-440c-a18f-17ce0095d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in c:\\users\\johni\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"numpy<2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcc574d-48ec-4de6-a1dd-96d7d449ea22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'submission.csv'\n",
      "        ID    class\n",
      "0        1    grass\n",
      "1        2  orchard\n",
      "2        3  orchard\n",
      "3        4    water\n",
      "4        5  orchard\n",
      "...    ...      ...\n",
      "2840  2841    water\n",
      "2841  2842    water\n",
      "2842  2843    water\n",
      "2843  2844    water\n",
      "2844  2845    water\n",
      "\n",
      "[2845 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load new data\n",
    "train_data = pd.read_csv(r'C:\\Users\\johni\\Downloads\\week2\\hacktrain.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\johni\\Downloads\\week2\\hacktest.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_ndvi(data):\n",
    "    # Extract NDVI columns (assuming format like '20200101_N')\n",
    "    ndvi_cols = [col for col in data.columns if col.endswith('_N')]\n",
    "    dates = [pd.to_datetime(col.split('_')[0]) for col in ndvi_cols]\n",
    "    \n",
    "    # Sort columns by date\n",
    "    sorted_cols = [x for _, x in sorted(zip(dates, ndvi_cols))]\n",
    "    sorted_dates = sorted(dates)\n",
    "    \n",
    "    # Create NDVI matrix\n",
    "    ndvi_matrix = data[sorted_cols].values\n",
    "    \n",
    "    # Interpolation for missing values\n",
    "    for i in range(ndvi_matrix.shape[0]):\n",
    "        valid_mask = ~np.isnan(ndvi_matrix[i])\n",
    "        if np.sum(valid_mask) > 1:\n",
    "            f = interp1d(np.where(valid_mask)[0], ndvi_matrix[i, valid_mask], \n",
    "                         kind='linear', fill_value='extrapolate')\n",
    "            ndvi_matrix[i] = f(np.arange(ndvi_matrix.shape[1]))\n",
    "        elif np.sum(valid_mask) == 1:\n",
    "            ndvi_matrix[i] = np.where(valid_mask, ndvi_matrix[i, valid_mask], ndvi_matrix[i, valid_mask])\n",
    "    \n",
    "    # Apply Savitzky-Golay filter\n",
    "    ndvi_matrix = savgol_filter(ndvi_matrix, window_length=5, polyorder=2, axis=1)\n",
    "    \n",
    "    return ndvi_matrix, sorted_dates\n",
    "\n",
    "# Feature engineering\n",
    "def extract_features(ndvi_matrix, dates):\n",
    "    features = {\n",
    "        'mean': np.mean(ndvi_matrix, axis=1),\n",
    "        'median': np.median(ndvi_matrix, axis=1),\n",
    "        'std': np.std(ndvi_matrix, axis=1),\n",
    "        'min': np.min(ndvi_matrix, axis=1),\n",
    "        'max': np.max(ndvi_matrix, axis=1),\n",
    "        'range': np.ptp(ndvi_matrix, axis=1),\n",
    "    }\n",
    "    \n",
    "    # Slope (trend)\n",
    "    x = np.arange(ndvi_matrix.shape[1])\n",
    "    slopes = []\n",
    "    for row in ndvi_matrix:\n",
    "        if np.all(~np.isnan(row)):\n",
    "            slope = np.polyfit(x, row, 1)[0]\n",
    "        else:\n",
    "            slope = np.nan\n",
    "        slopes.append(slope)\n",
    "    features['slope'] = slopes\n",
    "\n",
    "    # Seasonal statistics\n",
    "    seasons = []\n",
    "    for date in dates:\n",
    "        month = date.month\n",
    "        if month in [12, 1, 2]:\n",
    "            seasons.append('winter')\n",
    "        elif month in [3, 4, 5]:\n",
    "            seasons.append('spring')\n",
    "        elif month in [6, 7, 8]:\n",
    "            seasons.append('summer')\n",
    "        else:\n",
    "            seasons.append('fall')\n",
    "    \n",
    "    unique_seasons = list(set(seasons))\n",
    "    for season in unique_seasons:\n",
    "        season_mask = np.array([s == season for s in seasons])\n",
    "        features[f'mean_{season}'] = np.mean(ndvi_matrix[:, season_mask], axis=1)\n",
    "        features[f'std_{season}'] = np.std(ndvi_matrix[:, season_mask], axis=1)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Process training data\n",
    "X_train_ndvi, dates = preprocess_ndvi(train_data)\n",
    "X_train_features = extract_features(X_train_ndvi, dates)\n",
    "y_train = train_data['class']  # Make sure 'class' column exists\n",
    "\n",
    "# Process test data\n",
    "X_test_ndvi, _ = preprocess_ndvi(test_data)\n",
    "X_test_features = extract_features(X_test_ndvi, dates)\n",
    "\n",
    "# Model pipeline\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameter grid for logistic regression\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Grid search cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_features, y_train)\n",
    "\n",
    "# Best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions on test data\n",
    "test_preds = best_model.predict(X_test_features)\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],  # Make sure 'ID' column exists in test set\n",
    "    'class': test_preds\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved as 'submission.csv'\")\n",
    "print(submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b20df3c-d281-4feb-be92-8a7d8485fdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johni\\Downloads\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09b2ec-6c8f-4fcd-b17f-7e0f2751b3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
