{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104491,"databundleVersionId":12585144,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install \"numpy<2\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:50:14.425813Z","iopub.execute_input":"2025-06-11T17:50:14.426159Z","iopub.status.idle":"2025-06-11T17:50:14.431191Z","shell.execute_reply.started":"2025-06-11T17:50:14.426129Z","shell.execute_reply":"2025-06-11T17:50:14.429987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.signal import savgol_filter\nfrom scipy.interpolate import interp1d\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# Load new data\ntrain_data = pd.read_csv('/kaggle/input/summer-analytics-mid-hackathon/hacktrain.csv')\ntest_data = pd.read_csv('/kaggle/input/summer-analytics-mid-hackathon/hacktest.csv')\n\n# Preprocessing function\ndef preprocess_ndvi(data):\n    # Extract NDVI columns (assuming format like '20200101_N')\n    ndvi_cols = [col for col in data.columns if col.endswith('_N')]\n    dates = [pd.to_datetime(col.split('_')[0]) for col in ndvi_cols]\n    \n    # Sort columns by date\n    sorted_cols = [x for _, x in sorted(zip(dates, ndvi_cols))]\n    sorted_dates = sorted(dates)\n    \n    # Create NDVI matrix\n    ndvi_matrix = data[sorted_cols].values\n    \n    # Interpolation for missing values\n    for i in range(ndvi_matrix.shape[0]):\n        valid_mask = ~np.isnan(ndvi_matrix[i])\n        if np.sum(valid_mask) > 1:\n            f = interp1d(np.where(valid_mask)[0], ndvi_matrix[i, valid_mask], \n                         kind='linear', fill_value='extrapolate')\n            ndvi_matrix[i] = f(np.arange(ndvi_matrix.shape[1]))\n        elif np.sum(valid_mask) == 1:\n            ndvi_matrix[i] = np.where(valid_mask, ndvi_matrix[i, valid_mask], ndvi_matrix[i, valid_mask])\n    \n    # Apply Savitzky-Golay filter\n    ndvi_matrix = savgol_filter(ndvi_matrix, window_length=5, polyorder=2, axis=1)\n    \n    return ndvi_matrix, sorted_dates\n\n# Feature engineering\ndef extract_features(ndvi_matrix, dates):\n    features = {\n        'mean': np.mean(ndvi_matrix, axis=1),\n        'median': np.median(ndvi_matrix, axis=1),\n        'std': np.std(ndvi_matrix, axis=1),\n        'min': np.min(ndvi_matrix, axis=1),\n        'max': np.max(ndvi_matrix, axis=1),\n        'range': np.ptp(ndvi_matrix, axis=1),\n    }\n    \n    # Slope (trend)\n    x = np.arange(ndvi_matrix.shape[1])\n    slopes = []\n    for row in ndvi_matrix:\n        if np.all(~np.isnan(row)):\n            slope = np.polyfit(x, row, 1)[0]\n        else:\n            slope = np.nan\n        slopes.append(slope)\n    features['slope'] = slopes\n\n    # Seasonal statistics\n    seasons = []\n    for date in dates:\n        month = date.month\n        if month in [12, 1, 2]:\n            seasons.append('winter')\n        elif month in [3, 4, 5]:\n            seasons.append('spring')\n        elif month in [6, 7, 8]:\n            seasons.append('summer')\n        else:\n            seasons.append('fall')\n    \n    unique_seasons = list(set(seasons))\n    for season in unique_seasons:\n        season_mask = np.array([s == season for s in seasons])\n        features[f'mean_{season}'] = np.mean(ndvi_matrix[:, season_mask], axis=1)\n        features[f'std_{season}'] = np.std(ndvi_matrix[:, season_mask], axis=1)\n    \n    return pd.DataFrame(features)\n\n# Process training data\nX_train_ndvi, dates = preprocess_ndvi(train_data)\nX_train_features = extract_features(X_train_ndvi, dates)\ny_train = train_data['class']  # Make sure 'class' column exists\n\n# Process test data\nX_test_ndvi, _ = preprocess_ndvi(test_data)\nX_test_features = extract_features(X_test_ndvi, dates)\n\n# Model pipeline\npipeline = make_pipeline(\n    StandardScaler(),\n    LogisticRegression(max_iter=1000, class_weight='balanced')\n)\n\n\n\n\n# Parameter grid for logistic regression\nparam_grid = {\n    'logisticregression__C': [0.01, 0.1, 1, 10, 100]\n}\n\n# Grid search cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train_features, y_train)\n\n# Best model from grid search\nbest_model = grid_search.best_estimator_\n\n# Predictions on test data\ntest_preds = best_model.predict(X_test_features)\n\n# Prepare submission\nsubmission = pd.DataFrame({\n    'ID': test_data['ID'],  # Make sure 'ID' column exists in test set\n    'class': test_preds\n})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as 'submission.csv'\")\nprint(submission)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:50:14.432814Z","iopub.execute_input":"2025-06-11T17:50:14.433118Z","iopub.status.idle":"2025-06-11T17:50:34.817888Z","shell.execute_reply.started":"2025-06-11T17:50:14.433092Z","shell.execute_reply":"2025-06-11T17:50:34.816886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nprint(os.getcwd())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:50:34.819001Z","iopub.execute_input":"2025-06-11T17:50:34.819324Z","iopub.status.idle":"2025-06-11T17:50:34.827378Z","shell.execute_reply.started":"2025-06-11T17:50:34.819297Z","shell.execute_reply":"2025-06-11T17:50:34.824073Z"}},"outputs":[],"execution_count":null}]}